<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Debarghya Ghoshdastidar on Debarghya Ghoshdastidar</title>
    <link>/</link>
    <description>Recent content in Debarghya Ghoshdastidar on Debarghya Ghoshdastidar</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Theoretical Foundations of AI</title>
      <link>/group/hero/</link>
      <pubDate>Sun, 15 Oct 2017 00:00:00 +0200</pubDate>
      
      <guid>/group/hero/</guid>
      <description>&lt;p&gt;TUM Department of Informatics &lt;br&gt;
Technical University of Munich&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Focus</title>
      <link>/group/focus/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0200</pubDate>
      
      <guid>/group/focus/</guid>
      <description>&lt;p&gt;The research group works on the statistical theory of machine learning and artificial intelligence,
We provide mathematical understanding of learning algorithms, thereby establishing black-box AI tools as formal statistical principles. Our research focuses on three aspects of data science in the present era of big data analysis.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Data complexity&lt;/strong&gt;: Many applications involve complex data, such as relational information, texts etc., that often lack natural Euclidean structure. Hence, learning from such data require non-parametric approaches or representation learning.
We work with networks and ordinal data, and are continuously exploring applications with other complex unstructured data.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Statistical guarantee&lt;/strong&gt;: The ubiquity of non-standard learning problems has led to a demand for new paradigms in learning theory to analyse the performance of related learning algorithms.
We provide theoretical foundations for analysing algorithms in non-standard machine learning problems. These range from statistical performance guarantees to information theoretic limits of learning.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Computational cost&lt;/strong&gt;: The question of statistical performance is inherently tied with the computational budget, which has become important with the challenge of efficiently learning from massive volumes of complex data. We study the trade-off between efficiency and accuracy, and develop efficient machine learning algorithm based on sound statistical principles.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Members</title>
      <link>/group/members/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0200</pubDate>
      
      <guid>/group/members/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../#about&#34;&gt;Debarghya Ghoshdastidar&lt;/a&gt; (Professor)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www7.in.tum.de/people/detail/index.php?id=people.detail&amp;amp;arg=142&#34; target=&#34;_blank&#34;&gt;Claudia Link&lt;/a&gt; (Administration)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.tml.cs.uni-tuebingen.de/team/leena_chennuru/index.php&#34; target=&#34;_blank&#34;&gt;Leena Vankadara&lt;/a&gt;  (PhD student, jointly with &lt;a href=&#34;http://www.tml.cs.uni-tuebingen.de/team/luxburg/index.php&#34; target=&#34;_blank&#34;&gt;Ulrike von Luxburg&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Projects</title>
      <link>/group/projects/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0200</pubDate>
      
      <guid>/group/projects/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Foundations of comparison-based hierarchical clustering</title>
      <link>/publication/ghoshdastidar-neurips-2019/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/ghoshdastidar-neurips-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Two-sample hypothesis testing for inhomogeneous random graphs</title>
      <link>/publication/ghoshdastidar-aos-2019/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/ghoshdastidar-aos-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Practical methods for graph two-sample testing</title>
      <link>/publication/ghoshdastidar-nips-2018/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/ghoshdastidar-nips-2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Clustering Large Evolving Networks</title>
      <link>/project/network-clustering/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/network-clustering/</guid>
      <description>&lt;p&gt;One of the most significant problems in network sciences is clustering or community detection. It is a principle tool for exploratory data analysis and is also crucial for applications such as image processing, server load balancing, and several other areas. Theoretical guarantees on network clustering show that many popular methods are statistically accurate for very large networks. However, one often overlooks the fact that such methods are quite inefficient for large scale applications, and hence, practically efficient clustering algorithms deviate significantly from the theoretically studied methods. The purpose of this project is to blend theory with practice by developing efficient network clustering algorithms that have provable theoretical guarantees, and are also efficient for large scale applications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Two-sample tests for large random graphs using network statistics</title>
      <link>/publication/ghoshdastidar-colt-2017/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/ghoshdastidar-colt-2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Uniform hypergraph partitioning: Provable tensor methods and sampling techniques</title>
      <link>/publication/ghoshdastidar-jmlr-2017/</link>
      <pubDate>Mon, 01 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/ghoshdastidar-jmlr-2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Comparison based nearest neighbor search</title>
      <link>/publication/haghiri-aistats-2017/</link>
      <pubDate>Fri, 21 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/haghiri-aistats-2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Consistency of Spectral Hypergraph Partitioning under Planted Partition Model</title>
      <link>/publication/ghoshdastidar-aos-2017/</link>
      <pubDate>Wed, 01 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/ghoshdastidar-aos-2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Machine learning for ordinal data</title>
      <link>/project/ordinal/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/ordinal/</guid>
      <description>&lt;p&gt;The problem of learning from ordinal or comparison based information has its roots in psychology, and has gained importance with the popularity of crowdsourcing approaches. In a comparison based framework, one is given a set of items, but has no access to any Euclidean representation or any pairwise similarity / distance information. The only available information are binary responses to questions of the form: Are items &lt;em&gt;A&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt; most similar than items &lt;em&gt;C&lt;/em&gt; and &lt;em&gt;D&lt;/em&gt;? The significance of ordinal data lies in the fact that humans are better at providing preferences instead of stating absolute values, but its usefulness goes beyond this psychological phenomenon and ordinal data such as k-nearest neighbours are versatile tools in data engineering. With the increasing scope of crowdsourcing, it has become important to extend classical machine learning algorithms for classification, clustering etc. to the ordinal setting. Statistical theory of learning from ordinal data is in its early stages, and the project provides several foundational results in this setting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mixture modelling with compact support distributions for unsupervised learning</title>
      <link>/publication/dukkipati-ijcnn-2016/</link>
      <pubDate>Sun, 24 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/publication/dukkipati-ijcnn-2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hypothesis Testing for Networks</title>
      <link>/project/network-twosampletesting/</link>
      <pubDate>Sun, 01 May 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/network-twosampletesting/</guid>
      <description>&lt;p&gt;We study the problem of testing whether two large graphs are generated from the same model. This problem lies behind the question of whether brain network of Alzheimer patients differ from those of healthy individuals. On the applied end, we proposed new two-sample tests to compare graphs / graph populations. Our theory resolved a fundamental question â€“ &lt;em&gt;when can we test large networks without access to multiple independent samples?&lt;/em&gt; We used statistical minimax theory to show that solvability of a testing problem with limited data depends crucially on the hypotheses under consideration. We found that some classical approaches to testing can fail in this setup, and hence, one should be careful before relying on the test conclusion in this setup.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Codes are available &lt;a href=&#34;https://github.com/gdebarghya/Network-TwoSampleTesting&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
