<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Debarghya Ghoshdastidar on Debarghya Ghoshdastidar</title>
    <link>/</link>
    <description>Recent content in Debarghya Ghoshdastidar on Debarghya Ghoshdastidar</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Theoretical Foundations of AI</title>
      <link>/group/hero/</link>
      <pubDate>Sun, 15 Oct 2017 00:00:00 +0200</pubDate>
      
      <guid>/group/hero/</guid>
      <description>&lt;p&gt;TUM Department of Informatics &lt;br&gt;
Technical University of Munich&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Focus</title>
      <link>/group/focus/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0200</pubDate>
      
      <guid>/group/focus/</guid>
      <description>&lt;p&gt;The research group works on the statistical theory of machine learning and artificial intelligence,
We provide mathematical understanding of learning algorithms, thereby establishing black-box AI tools as formal statistical principles. Our research focuses on three aspects of data science in the present era of big data analysis.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Data complexity&lt;/strong&gt;: Many applications involve complex data, such as relational information, texts etc., that often lack natural Euclidean structure. Hence, learning from such data require non-parametric approaches or representation learning.
We work with networks and ordinal data, and are continuously exploring applications with other complex unstructured data.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Statistical guarantee&lt;/strong&gt;: The ubiquity of non-standard learning problems has led to a demand for new paradigms in learning theory to analyse the performance of related learning algorithms.
We provide theoretical foundations for analysing algorithms in non-standard machine learning problems. These range from statistical performance guarantees to information theoretic limits of learning.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Computational cost&lt;/strong&gt;: The question of statistical performance is inherently tied with the computational budget, which has become important with the challenge of efficiently learning from massive volumes of complex data. We study the trade-off between efficiency and accuracy, and develop efficient machine learning algorithm based on sound statistical principles.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Members</title>
      <link>/group/members/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0200</pubDate>
      
      <guid>/group/members/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../#about&#34;&gt;Debarghya Ghoshdastidar&lt;/a&gt; (Professor)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www7.in.tum.de/people/detail/index.php?id=people.detail&amp;amp;arg=142&#34; target=&#34;_blank&#34;&gt;Claudia Link&lt;/a&gt; (Administration)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.tml.cs.uni-tuebingen.de/team/leena_chennuru/index.php&#34; target=&#34;_blank&#34;&gt;Leena Vankadara&lt;/a&gt;  (PhD student, jointly with &lt;a href=&#34;http://www.tml.cs.uni-tuebingen.de/team/luxburg/index.php&#34; target=&#34;_blank&#34;&gt;Ulrike von Luxburg&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Projects</title>
      <link>/group/projects/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0200</pubDate>
      
      <guid>/group/projects/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Research opportunities</title>
      <link>/group/jobs/</link>
      <pubDate>Fri, 06 Sep 2019 00:00:00 +0200</pubDate>
      
      <guid>/group/jobs/</guid>
      <description>

&lt;h2 id=&#34;open-positions&#34;&gt;Open positions&lt;/h2&gt;

&lt;h3 id=&#34;full-time-phd-student&#34;&gt;Full-time PhD student&lt;/h3&gt;

&lt;p&gt;We are looking for a PhD student, who is eager to work on the theoretical aspects of machine learning. The research would be related to one of the following topics:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Theory of deep learning (potentially Graph Neural Networks)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Communities and hierarchies in large random networks&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hypothesis testing and inference for high-dimensional data&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The position is a full time position on the German pay scale E13 (100%).
The initial contract would be for 3 years with potential extension. A successful candidate should have a masters degree in computer science, mathematics  or related areas. The position can start immediately.  See below for the application procedure.&lt;/p&gt;

&lt;h3 id=&#34;internship&#34;&gt;Internship&lt;/h3&gt;

&lt;p&gt;We have an open 3 month internship in our group. The project is about &lt;em&gt;similarity measures between graphs&lt;/em&gt;. The funding only covers the internship, and hence, it is suitable for students in Munich or T&amp;uuml;bingen.&lt;/p&gt;

&lt;h3 id=&#34;other-funding-opportunities&#34;&gt;Other funding opportunities&lt;/h3&gt;

&lt;p&gt;We are always looking for people interested in working on machine learning and its theory. If there are no advertised openings, you may still join our group with &lt;a href=&#34;https://www.daad.de/deutschland/stipendium/datenbank/en/21148-scholarship-database/&#34; target=&#34;_blank&#34;&gt;others scholarships&lt;/a&gt;. Please follow the application procedure for PhD and provide your funding information.&lt;/p&gt;

&lt;h2 id=&#34;how-to-apply&#34;&gt;How to apply?&lt;/h2&gt;

&lt;h3 id=&#34;master-bachelor-thesis&#34;&gt;Master / bachelor thesis&lt;/h3&gt;

&lt;p&gt;For students in TUM, send me an email if you are interested in writing your thesis on machine learning or learning theory. Please include details of your study program, department etc.&lt;/p&gt;

&lt;p&gt;For others who want to write their thesis with our group, you need to have a supervisor in your host institute. If you are applying from outside Germany, please follow the application procedure for PhD applicants.&lt;/p&gt;

&lt;h3 id=&#34;phd-postdoc-internship&#34;&gt;PhD, postdoc, internship&lt;/h3&gt;

&lt;p&gt;Please send the following documents by email to Prof. Ghoshdastidar. We cannot respond to emails that do not contain all documents listed below.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Single PDF file containing following details&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Letter explaining why you want to join our group (max 2 pages)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Curriculum vitae, including list of publications (if applicable)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Contact details of two referees (link to homepage, email address). Please do not attach their reference letters to your application.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Certificates of master and bachelor degrees, school leaving examination, and all transcript of records.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A second PDF file containing one of your publications or latest thesis&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If your application interests us, we will contact you within three weeks.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Foundations of comparison-based hierarchical clustering</title>
      <link>/publication/ghoshdastidar-neurips-2019/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/ghoshdastidar-neurips-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Two-sample hypothesis testing for inhomogeneous random graphs</title>
      <link>/publication/ghoshdastidar-aos-2019/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/ghoshdastidar-aos-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Practical methods for graph two-sample testing</title>
      <link>/publication/ghoshdastidar-nips-2018/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/ghoshdastidar-nips-2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Clustering Large Evolving Networks</title>
      <link>/project/network-clustering/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/network-clustering/</guid>
      <description>&lt;p&gt;One of the most significant problems in network sciences is clustering or community detection. It is a principle tool for exploratory data analysis and is also crucial for applications such as image processing, server load balancing, and several other areas. Theoretical guarantees on network clustering show that many popular methods are statistically accurate for very large networks. However, one often overlooks the fact that such methods are quite inefficient for large scale applications, and hence, practically efficient clustering algorithms deviate significantly from the theoretically studied methods. The purpose of this project is to blend theory with practice by developing efficient network clustering algorithms that have provable theoretical guarantees, and are also efficient for large scale applications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Two-sample tests for large random graphs using network statistics</title>
      <link>/publication/ghoshdastidar-colt-2017/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/ghoshdastidar-colt-2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Uniform hypergraph partitioning: Provable tensor methods and sampling techniques</title>
      <link>/publication/ghoshdastidar-jmlr-2017/</link>
      <pubDate>Mon, 01 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/ghoshdastidar-jmlr-2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Comparison based nearest neighbor search</title>
      <link>/publication/haghiri-aistats-2017/</link>
      <pubDate>Fri, 21 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/haghiri-aistats-2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Consistency of Spectral Hypergraph Partitioning under Planted Partition Model</title>
      <link>/publication/ghoshdastidar-aos-2017/</link>
      <pubDate>Wed, 01 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/ghoshdastidar-aos-2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Machine learning for ordinal data</title>
      <link>/project/ordinal/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/ordinal/</guid>
      <description>&lt;p&gt;The problem of learning from ordinal or comparison based information has its roots in psychology, and has gained importance with the popularity of crowdsourcing approaches. In a comparison based framework, one is given a set of items, but has no access to any Euclidean representation or any pairwise similarity / distance information. The only available information are binary responses to questions of the form: Are items &lt;em&gt;A&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt; most similar than items &lt;em&gt;C&lt;/em&gt; and &lt;em&gt;D&lt;/em&gt;? The significance of ordinal data lies in the fact that humans are better at providing preferences instead of stating absolute values, but its usefulness goes beyond this psychological phenomenon and ordinal data such as k-nearest neighbours are versatile tools in data engineering. With the increasing scope of crowdsourcing, it has become important to extend classical machine learning algorithms for classification, clustering etc. to the ordinal setting. Statistical theory of learning from ordinal data is in its early stages, and the project provides several foundational results in this setting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mixture modelling with compact support distributions for unsupervised learning</title>
      <link>/publication/dukkipati-ijcnn-2016/</link>
      <pubDate>Sun, 24 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/publication/dukkipati-ijcnn-2016/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
