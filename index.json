[{"authors":null,"categories":null,"content":"Theory of Machine Learning (TML) Group Department of Computer Science University of Tübingen\n","date":1508018400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508018400,"objectID":"b16970129ca3963a2d4f3feb3ccb0f93","permalink":"/group/hero/","publishdate":"2017-10-15T00:00:00+02:00","relpermalink":"/group/hero/","section":"group","summary":"Theory of Machine Learning (TML) Group Department of Computer Science University of Tübingen","tags":null,"title":"TML Network Analysis Team","type":"group"},{"authors":null,"categories":null,"content":"The Network Analysis Team primarily works on the theory and applications of machine learning with networks. We develop network analysis tools for various applied problems ranging from computational neuroscience to production engineering. We also study the statistical theory of machine learning with networks.\n","date":1461103200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461103200,"objectID":"558045126a3dbe846f71e00ab4bf31ee","permalink":"/group/focus/","publishdate":"2016-04-20T00:00:00+02:00","relpermalink":"/group/focus/","section":"group","summary":"The Network Analysis Team primarily works on the theory and applications of machine learning with networks. We develop network analysis tools for various applied problems ranging from computational neuroscience to production engineering. We also study the statistical theory of machine learning with networks.","tags":null,"title":"Research Focus","type":"group"},{"authors":null,"categories":null,"content":" Debarghya Ghoshdastidar (Postdoc; Team Leader) Leena Chennuru Vankadara (PhD Student)  ","date":1461103200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461103200,"objectID":"f0668a09660bf0e839e3f7c3bb83cd6e","permalink":"/group/members/","publishdate":"2016-04-20T00:00:00+02:00","relpermalink":"/group/members/","section":"group","summary":" Debarghya Ghoshdastidar (Postdoc; Team Leader) Leena Chennuru Vankadara (PhD Student)  ","tags":null,"title":"Members","type":"group"},{"authors":null,"categories":null,"content":"","date":1461103200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461103200,"objectID":"2845a47d7f823f834c9903d0af74ecc7","permalink":"/group/projects/","publishdate":"2016-04-20T00:00:00+02:00","relpermalink":"/group/projects/","section":"group","summary":"","tags":null,"title":"Projects","type":"group"},{"authors":["D. Ghoshdastidar","M. Perrot","U. Von Luxburg"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"9d0c52222ab3898f22d80186bb50f075","permalink":"/publication/ghoshdastidar-neurips-2019/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/publication/ghoshdastidar-neurips-2019/","section":"publication","summary":"We address the classical problem of hierarchical clustering, but in a framework where one does not have access to a representation of the objects or their pairwise similarities. Instead, we assume that only a set of comparisons between objects is available, that is, statements of the form: objects *i* and *j* are more similar than objects *k* and *l*. Such a scenario is commonly encountered in crowdsourcing applications. The focus of this work is to develop comparison-based hierarchical clustering algorithms that do not rely on the principles of ordinal embedding. We show that single and complete linkage are inherently comparison-based and we develop variants of average linkage. We provide statistical guarantees for the different methods under a planted hierarchical partition model. We also empirically demonstrate the performance of the proposed approaches on several datasets.","tags":[],"title":"Foundations of comparison-based hierarchical clustering","type":"publication"},{"authors":["D. Ghoshdastidar","M. Gutzeit","A. Carpentier","U. Von Luxburg"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"9cc958679a13d4cfbd738c8412f65f7d","permalink":"/publication/ghoshdastidar-aos-2019/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/ghoshdastidar-aos-2019/","section":"publication","summary":"The study of networks leads to a wide range of high dimensional inference problems. In many practical applications, one needs to draw inference from one or few large sparse networks. The present paper studies hypothesis testing of graphs in this high-dimensional regime, where the goal is to test between two populations of inhomogeneous random graphs defined on the same set of $n$ vertices. The size of each population $m$ is much smaller than $n$, and can even be a constant as small as 1. The critical question in this context is whether the problem is solvable for small $m$. We answer this question from a minimax testing perspective. Let $P,Q$ be the population adjacencies of two sparse inhomogeneous random graph models, and $d$ be a suitably defined distance function. Given a population of $m$ graphs from each model, we derive minimax separation rates for the problem of testing $P=Q$ against $d(P,Q)\\rho$. We observe that if $m$ is small, then the minimax separation is too large for some popular choices of $d$, including total variation distance between corresponding distributions. This implies that some models that are widely separated in $d$ cannot be distinguished for small $m$, and hence, the testing problem is generally not solvable in these cases. We also show that if $m1$, then the minimax separation is relatively small if $d$ is the Frobenius norm or operator norm distance between $P$ and $Q$. For $m=1$, only the latter distance provides small minimax separation. Thus, for these distances, the problem is solvable for small $m$. We also present near-optimal two-sample tests in both cases, where tests are adaptive with respect to sparsity level of the graphs.","tags":[],"title":"Two-sample hypothesis testing for inhomogeneous random graphs","type":"publication"},{"authors":["D. Ghoshdastidar","U. Von Luxburg"],"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"8aa642dd03eaa01e9474c955d678586e","permalink":"/publication/ghoshdastidar-nips-2018/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/publication/ghoshdastidar-nips-2018/","section":"publication","summary":"Hypothesis testing for graphs has been an important tool in several applied research fields for more than two decades, and still remains a challenging problem as one often needs to draw inference from few replicates of large graphs. Recent studies in statistics and learning theory have provided some theoretical insights about such high-dimensional graph testing problems, but the practicality of the developed theoretical methods remains an open question. In this paper, we consider the problem of two-sample testing of large graphs. We demonstrate the practical merits and limitations of existing theoretical tests, or more precisely, their bootstrapped variants. We also propose two new tests based on asymptotic distributions, and show that the proposed tests are computationally less expensive and, in some cases, more reliable than the existing methods.","tags":[],"title":"Practical methods for graph two-sample testing","type":"publication"},{"authors":null,"categories":null,"content":"One of the most significant problems in network sciences is clustering or community detection. It is a principle tool for exploratory data analysis and is also crucial for applications such as image processing, server load balancing, and several other areas. Theoretical guarantees on network clustering show that many popular methods are statistically accurate for very large networks. However, one often overlooks the fact that such methods are quite inefficient for large scale applications, and hence, practically efficient clustering algorithms deviate significantly from the theoretically studied methods. The purpose of this project is to blend theory with practice by developing efficient network clustering algorithms that have provable theoretical guarantees, and are also efficient for large scale applications.\n","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"5c6b98d7031de97c62031b088b59ef53","permalink":"/project/network-clustering/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/project/network-clustering/","section":"project","summary":"Ongoing since 2018","tags":["machine-learning","network-analysis"],"title":"Clustering Large Evolving Networks","type":"project"},{"authors":["D. Ghoshdastidar","M. Gutzeit","A. Carpentier","U. von Luxburg"],"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"cc223cc9d529b181b10cb725b2de97b7","permalink":"/publication/ghoshdastidar-colt-2017/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/publication/ghoshdastidar-colt-2017/","section":"publication","summary":"We consider a two-sample hypothesis testing problem, where the distributions are defined on the space of undirected graphs, and one has access to only one observation from each model. A motivating example for this problem is comparing the friendship networks on Facebook and LinkedIn. The practical approach to such problems is to compare the networks based on certain network statistics. In this paper, we present a general principle for two-sample hypothesis testing in such scenarios without making any assumption about the network generation process. The main contribution of the paper is a general formulation of the problem based on concentration of network statistics, and consequently, a consistent two-sample test that arises as the natural solution for this problem. We also show that the proposed test is minimax optimal for certain network statistics.","tags":[],"title":"Two-sample tests for large random graphs using network statistics","type":"publication"},{"authors":["D. Ghoshdastidar","A. Dukkipati"],"categories":null,"content":"","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493596800,"objectID":"854e461fa34742c11d68e5a0f33596aa","permalink":"/publication/ghoshdastidar-jmlr-2017/","publishdate":"2017-05-01T00:00:00Z","relpermalink":"/publication/ghoshdastidar-jmlr-2017/","section":"publication","summary":"In a series of recent works, we have generalised the consistency results in the stochastic block model literature to the case of uniform and non-uniform hypergraphs. The present paper continues the same line of study, where we focus on partitioning weighted uniform hypergraphs---a problem often encountered in computer vision. This work is motivated by two issues that arise when a hypergraph partitioning approach is used to tackle computer vision problems: (i) The uniform hypergraphs constructed for higher-order learning contain all edges, but most have negligible weights. Thus, the adjacency tensor is nearly sparse, and yet, not binary. (ii) A more serious concern is that standard partitioning algorithms need to compute all edge weights, which is computationally expensive for hypergraphs. This is usually resolved in practice by merging the clustering algorithm with a tensor sampling strategy---an approach that is yet to be analysed rigorously. We build on our earlier work on partitioning dense unweighted uniform hypergraphs (Ghoshdastidar and Dukkipati, ICML, 2015), and address the aforementioned issues by proposing provable and efficient partitioning algorithms. Our analysis justifies the empirical success of practical sampling techniques. We also complement our theoretical findings by elaborate empirical comparison of various hypergraph partitioning schemes.","tags":[],"title":"Uniform hypergraph partitioning: Provable tensor methods and sampling techniques","type":"publication"},{"authors":["S. Haghiri","D. Ghoshdastidar","U. von Luxburg"],"categories":null,"content":"","date":1492732800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1492732800,"objectID":"6cd294a820ef695f976727345a5c40f1","permalink":"/publication/haghiri-aistats-2017/","publishdate":"2017-04-21T00:00:00Z","relpermalink":"/publication/haghiri-aistats-2017/","section":"publication","summary":"We consider machine learning in a comparison-based setting where we are given a set of points in a metric space, but we have no access to the actual distances between the points. Instead, we can only ask an oracle whether the distance between two points i and j is smaller than the distance between the points i and k. We are concerned with data structures and algorithms to find nearest neighbors based on such comparisons. We focus on a simple yet effective algorithm that recursively splits the space by first selecting two random pivot points and then assigning all other points to the closer of the two (comparison tree). We prove that if the metric space satisfies certain expansion conditions, then with high probability the height of the comparison tree is logarithmic in the number of points, leading to efficient search performance. We also provide an upper bound for the failure probability to return the true nearest neighbor. Experiments show that the comparison tree is competitive with algorithms that have access to the actual distance values, and needs less triplet comparisons than other competitors.","tags":[],"title":"Comparison based nearest neighbor search","type":"publication"},{"authors":["D. Ghoshdastidar","A. Dukkipati"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485907200,"objectID":"acab59488ec8c12f6d737ce732c77b60","permalink":"/publication/ghoshdastidar-aos-2017/","publishdate":"2017-02-01T00:00:00Z","relpermalink":"/publication/ghoshdastidar-aos-2017/","section":"publication","summary":"Hypergraph partitioning lies at the heart of a number of problems in machine learning and network sciences. Many algorithms for hypergraph partitioning have been proposed that extend standard approaches for graph partitioning to the case of hypergraphs. However, theoretical aspects of such methods have seldom received attention in the literature as compared to the extensive studies on the guarantees of graph partitioning. For instance, consistency results of spectral graph partitioning under the stochastic block model are well known. In this paper, we present a planted partition model for sparse random non-uniform hypergraphs that generalizes the stochastic block model. We derive an error bound for a spectral hypergraph partitioning algorithm under this model using matrix concentration inequalities. To the best of our knowledge, this is the first consistency result related to partitioning non-uniform hypergraphs.","tags":[],"title":"Consistency of Spectral Hypergraph Partitioning under Planted Partition Model","type":"publication"},{"authors":null,"categories":null,"content":"The problem of learning from ordinal or comparison based information has its roots in psychology, and has gained importance with the popularity of crowdsourcing approaches. In a comparison based framework, one is given a set of items, but has no access to any Euclidean representation or any pairwise similarity / distance information. The only available information are binary responses to questions of the form: Are items A and B most similar than items C and D? The significance of ordinal data lies in the fact that humans are better at providing preferences instead of stating absolute values, but its usefulness goes beyond this psychological phenomenon and ordinal data such as k-nearest neighbours are versatile tools in data engineering. With the increasing scope of crowdsourcing, it has become important to extend classical machine learning algorithms for classification, clustering etc. to the ordinal setting. Statistical theory of learning from ordinal data is in its early stages, and the project provides several foundational results in this setting.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"a128376b6b8c29260c7b1cfbc57194a0","permalink":"/project/ordinal/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/project/ordinal/","section":"project","summary":"Ongoing since 2017","tags":["machine-learning","statistics"],"title":"Machine learning for ordinal data","type":"project"},{"authors":["A. Dukkipati","D. Ghoshdastidar","J. Krishnan"],"categories":null,"content":"","date":1469318400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1469318400,"objectID":"e9e04c1085c0da4579191ea33b3d4885","permalink":"/publication/dukkipati-ijcnn-2016/","publishdate":"2016-07-24T00:00:00Z","relpermalink":"/publication/dukkipati-ijcnn-2016/","section":"publication","summary":"The importance of the q-Gaussian distributions is attributed to their power law nature and the fact that they generalize the Gaussian distributions (q → 1 retrieves the Gaussian distributions). While for q  1, a q-Gaussian distribution is nothing but a Student's t-distribution, which is a long tailed distribution, for q ","tags":[],"title":"Mixture modelling with compact support distributions for unsupervised learning","type":"publication"},{"authors":null,"categories":null,"content":"We study the problem of testing whether two large graphs are generated from the same model. This problem lies behind the question of whether brain network of Alzheimer patients differ from those of healthy individuals. On the applied end, we proposed new two-sample tests to compare graphs / graph populations. Our theory resolved a fundamental question – when can we test large networks without access to multiple independent samples? We used statistical minimax theory to show that solvability of a testing problem with limited data depends crucially on the hypotheses under consideration. We found that some classical approaches to testing can fail in this setup, and hence, one should be careful before relying on the test conclusion in this setup.\nCodes are available here\n","date":1462060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462060800,"objectID":"2818c1d3895bd67d78a3263581e11804","permalink":"/project/network-twosampletesting/","publishdate":"2016-05-01T00:00:00Z","relpermalink":"/project/network-twosampletesting/","section":"project","summary":"2016-2019","tags":["machine-learning","network-analysis","statistics"],"title":"Hypothesis Testing for Networks","type":"project"},{"authors":["D. Ghoshdastidar","A. P. Adsul","A. Dukkipati"],"categories":null,"content":"","date":1460592000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1460592000,"objectID":"bf6840b7d0c0c8dc80947f679e649bfc","permalink":"/publication/ghoshdastidar-tnnls-2016/","publishdate":"2016-04-14T00:00:00Z","relpermalink":"/publication/ghoshdastidar-tnnls-2016/","section":"publication","summary":"Jensen-type [Jensen-Shannon (JS) and Jensen-Tsallis] kernels were first proposed by Martins et al. (2009). These kernels are based on JS divergences that originated in the information theory. In this paper, we extend the Jensen-type kernels on probability measures to define positive-definite kernels on Euclidean space. We show that the special cases of these kernels include dot-product kernels. Since Jensen-type divergences are multidistribution divergences, we propose their multipoint variants, and study spectral clustering and kernel methods based on these. We also provide experimental studies on benchmark image database and gene expression database that show the benefits of the proposed kernels compared with the existing kernels. The experiments on clustering also demonstrate the use of constructing multipoint similarities.","tags":[],"title":"Learning with Jensen-Tsallis kernels","type":"publication"},{"authors":["D. Ghoshdastidar","A. Dukkipati"],"categories":null,"content":"","date":1435795200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435795200,"objectID":"c5796d8e1d69bf51c6b7887b42530288","permalink":"/publication/ghoshdastidar-arxiv-00763/","publishdate":"2015-07-02T00:00:00Z","relpermalink":"/publication/ghoshdastidar-arxiv-00763/","section":"publication","summary":"Let $H_{n,(p_2,\\ldots,p_M)}$ be a random non-uniform hypergraph of dimension $M$ on $2n$ vertices, where the vertices are split into two disjoint sets of size $n$, and colored by two distinct colors. Each non-monochromatic edge of size $m=2,\\ldots,M$ is independently added with probability $p_m$. We show that if $p_2,\\ldots,p_M$ are such that the expected number of edges in the hypergraph is at least $dn\\ln n$, for some $d0$ sufficiently large, then with probability $(1-o(1))$, one can find a proper 2-coloring of the hypergraph in polynomial time. We present a polynomial time algorithm for hypergraph 2-coloring, and provide discussions on extension of the approach for $k$-coloring of non-uniform hypergraphs.","tags":[],"title":"Coloring random non-uniform bipartite hypergraphs","type":"publication"},{"authors":["D. Ghoshdastidar","A. Dukkipati"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"8bff2cd1b17ed6ad116fde62a9d5abf6","permalink":"/publication/ghoshdastidar-icml-2015/","publishdate":"2015-07-01T00:00:00Z","relpermalink":"/publication/ghoshdastidar-icml-2015/","section":"publication","summary":"Matrix spectral methods play an important role in statistics and machine learning, and most often the word ‘matrix’ is dropped as, by default, one assumes that similarities or affinities are measured between two points, thereby resulting in similarity matrices. However, recent challenges in computer vision and text mining have necessitated the use of multi-way affinities in the learning methods, and this has led to a considerable interest in hypergraph partitioning methods in machine learning community. A plethora of “higher-order” algorithms have been proposed in the past decade, but their theoretical guarantees are not well-studied. In this paper, we develop a unified approach for partitioning uniform hypergraphs by means of a tensor trace optimization problem involving the affinity tensor, and a number of existing higher-order methods turn out to be special cases of the proposed formulation. We further propose an algorithm to solve the proposed trace optimization problem, and prove that it is consistent under a planted hypergraph model. We also provide experimental results to validate our theoretical findings.","tags":[],"title":"A provable generalized tensor spectral method for uniform hypergraph partitioning","type":"publication"},{"authors":["D. Ghoshdastidar","A. Dukkipati"],"categories":null,"content":"","date":1422144000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1422144000,"objectID":"ce90665e020461b1e4a18e189cc630db","permalink":"/publication/ghoshdastidar-aaai-2015/","publishdate":"2015-01-25T00:00:00Z","relpermalink":"/publication/ghoshdastidar-aaai-2015/","section":"publication","summary":"Spectral clustering, a graph partitioning technique, has gained immense popularity in machine learning in the context of unsupervised learning. This is due to convincing empirical studies, elegant approaches involved and the theoretical guarantees provided in the literature. To tackle some challenging problems that arose in computer vision etc., recently, a need to develop spectral methods that incorporate multi-way similarity measures surfaced. This, in turn, leads to a hypergraph partitioning problem. In this paper, we formulate a criterion for partitioning uniform hypergraphs, and show that a relaxation of this problem is related to the multilinear singular value decomposition (SVD) of symmetric tensors. Using this, we provide a spectral technique for clustering based on higher order affinities, and derive a theoretical bound on the error incurred by this method. We also study the complexity of the algorithm and use Nystr ̈om’s method and column sampling techniques to develop approximate methods with significantly reduced complexity. Experiments on geometric grouping and motion segmentation demonstrate the practical significance of the proposed methods.","tags":[],"title":"Spectral clustering using multilinear SVD: Analysis, approximations and applications","type":"publication"},{"authors":["D. Ghoshdastidar","A. Dukkipati"],"categories":null,"content":"","date":1417392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1417392000,"objectID":"00d648e2202c823e88f83821ea6ce27d","permalink":"/publication/ghoshdastidar-nips-2014/","publishdate":"2014-12-01T00:00:00Z","relpermalink":"/publication/ghoshdastidar-nips-2014/","section":"publication","summary":"Spectral graph partitioning methods have received significant attention from both practitioners and theorists in computer science. Some notable studies have been carried out regarding the behavior of these methods for infinitely large sample size (von Luxburg et al., 2008; Rohe et al., 2011), which provide sufficient confidence to practitioners about the effectiveness of these methods. On the other hand, recent developments in computer vision have led to a plethora of applications, where the model deals with multi-way affinity relations and can be posed as uniform hyper-graphs. In this paper, we view these models as random m-uniform hypergraphs and establish the consistency of spectral algorithm in this general setting. We develop a planted partition model or stochastic blockmodel for such problems using higher order tensors, present a spectral technique suited for the purpose and study its large sample behavior. The analysis reveals that the algorithm is consistent for m-uniform hypergraphs for larger values of m, and also the rate of convergence improves for increasing m. Our result provides the first theoretical evidence that establishes the importance of m-way affinities.","tags":[],"title":"Consistency of spectral partitioning of uniform hypergraphs under planted partition model","type":"publication"},{"authors":["D. Ghoshdastidar","A. Dukkipati","S. Bhatnagar"],"categories":null,"content":"","date":1410307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1410307200,"objectID":"2d15e2cc8353786864e884aa7904326a","permalink":"/publication/ghoshdastidar-automatica-2014/","publishdate":"2014-09-10T00:00:00Z","relpermalink":"/publication/ghoshdastidar-automatica-2014/","section":"publication","summary":"We present the first $q$-Gaussian smoothed functional (SF) estimator of the Hessian and the first Newton-based stochastic optimization algorithm that estimates both the Hessian and the gradient of the objective function using $q$-Gaussian perturbations. Our algorithm requires only two system simulations (regardless of the parameter dimension) and estimates both the gradient and the Hessian at each update epoch using these. We also present a proof of convergence of the proposed algorithm. In a related recent work (Ghoshdastidar, Dukkipati, \u0026 Bhatnagar, 2014), we presented gradient SF algorithms based on the $q$-Gaussian perturbations. Our work extends prior work on SF algorithms by generalizing the class of perturbation distributions as most distributions reported in the literature for which SF algorithms are known to work turn out to be special cases of the $q$-Gaussian distribution. Besides studying the convergence properties of our algorithm analytically, we also show the results of numerical simulations on a model of a queuing network, that illustrate the significance of the proposed method. In particular, we observe that our algorithm performs better in most cases, over a wide range of $q$-values, in comparison to Newton SF algorithms with the Gaussian and Cauchy perturbations, as well as the gradient $q$-Gaussian SF algorithms.","tags":[],"title":"Newton based stochastic optimization using q-Gaussian smoothed functional algorithms","type":"publication"},{"authors":["D. Ghoshdastidar","A. Dukkipati","A. P. Adsul","A. S. Vijayan"],"categories":null,"content":"","date":1403481600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1403481600,"objectID":"277ca3fb392edec15fb9cfbd260f50bb","permalink":"/publication/ghoshdastidar-cvpr-2014/","publishdate":"2014-06-23T00:00:00Z","relpermalink":"/publication/ghoshdastidar-cvpr-2014/","section":"publication","summary":"Motivated by multi-distribution divergences, which originate in information theory, we propose a notion of 'multi-point' kernels, and study their applications. We study a class of kernels based on Jensen type divergences and show that these can be extended to measure similarity among multiple points. We study tensor flattening methods and develop a multi-point (kernel) spectral clustering (MSC) method. We further emphasize on a special case of the proposed kernels, which is a multi-point extension of the linear (dot-product) kernel and show the existence of cubic time tensor flattening algorithm in this case. Finally, we illustrate the usefulness of our contributions using standard data sets and image segmentation tasks.","tags":[],"title":"Spectral clustering with Jensen-type kernels and their multi-point extensions","type":"publication"},{"authors":["D. Ghoshdastidar","A. Dukkipati","S. Bhatnagar"],"categories":null,"content":"","date":1398902400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1398902400,"objectID":"d5e99fd0823141f6c472ec504016aceb","permalink":"/publication/ghoshdastidar-tomacs-2014/","publishdate":"2014-05-01T00:00:00Z","relpermalink":"/publication/ghoshdastidar-tomacs-2014/","section":"publication","summary":"Smoothed functional (SF) schemes for gradient estimation are known to be efficient in stochastic optimization algorithms, especially when the objective is to improve the performance of a stochastic system. However, the performance of these methods depends on several parameters, such as the choice of a suitable smoothing kernel. Different kernels have been studied in the literature, which include Gaussian, Cauchy, and uniform distributions, among others. This article studies a new class of kernels based on the q-Gaussian distribution, which has gained popularity in statistical physics over the last decade. Though the importance of this family of distributions is attributed to its ability to generalize the Gaussian distribution, we observe that this class encompasses almost all existing smoothing kernels. This motivates us to study SF schemes for gradient estimation using the q-Gaussian distribution. Using the derived gradient estimates, we propose two-timescale algorithms for optimization of a stochastic objective function in a constrained setting with a projected gradient search approach. We prove the convergence of our algorithms to the set of stationary points of an associated ODE. We also demonstrate their performance numerically through simulations on a queuing model.","tags":[],"title":"Smoothed functional algorithms for stochastic optimization using q-Gaussian distributions","type":"publication"},{"authors":["A. Dukkipati","G. Pandey","D. Ghoshdastidar","P. Koley","D. M. V. Satya Sriram"],"categories":null,"content":"","date":1386374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1386374400,"objectID":"e17d266d320dc0e3cd22bf9dac31dff6","permalink":"/publication/dukkipati-icdm-2013/","publishdate":"2013-12-07T00:00:00Z","relpermalink":"/publication/dukkipati-icdm-2013/","section":"publication","summary":"Maximum entropy approach to classification is very well studied in applied statistics and machine learning and almost all the methods that exists in literature are discriminative in nature. In this paper, we introduce a maximum entropy classification method with feature selection for large dimensional data such as text datasets that is generative in nature. To tackle the curse of dimensionality of large data sets, we employ conditional independence assumption (Naive Bayes) and we perform feature selection simultaneously, by enforcing a 'maximum discrimination' between estimated class conditional densities. For two class problems, in the proposed method, we use Jeffreys (J) divergence to discriminate the class conditional densities. To extend our method to the multi-class case, we propose a completely new approach by considering a multi-distribution divergence: we replace Jeffreys divergence by Jensen-Shannon (JS) divergence to discriminate conditional densities of multiple classes. In order to reduce computational complexity, we employ a modified Jensen-Shannon divergence (JS_GM), based on AM-GM inequality. We show that the resulting divergence is a natural generalization of Jeffreys divergence to a multiple distributions case. As far as the theoretical justifications are concerned we show that when one intends to select the best features in a generative maximum entropy approach, maximum discrimination using J-divergence emerges naturally in binary classification. Performance and comparative study of the proposed algorithms have been demonstrated on large dimensional text and gene expression datasets that show our methods scale up very well with large dimensional datasets.","tags":[],"title":"Generative maximum entropy learning for multiclass classification","type":"publication"},{"authors":["D. Ghoshdastidar","A. Dukkipati"],"categories":null,"content":"","date":1372550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372550400,"objectID":"ff7f51bb513a01bfc25511b35c504393","permalink":"/publication/ghoshdastidar-aaai-2013/","publishdate":"2013-06-30T00:00:00Z","relpermalink":"/publication/ghoshdastidar-aaai-2013/","section":"publication","summary":"The role of kernels is central to machine learning. Motivated by the importance of power-law distributions in statistical modeling, in this paper, we propose the notion of power-law kernels to investigate power-laws in learning problem. We propose two power-law kernels by generalizing Gaussian and Laplacian kernels. This generalization is based on distributions, arising out of maximization of a generalized information measure known as nonextensive entropy that is very well studied in statistical mechanics. We prove that the proposed kernels are positive definite, and provide some insights regarding the corresponding Reproducing Kernel Hilbert Space (RKHS). We also study practical significance of both kernels in classification and regression, and present some simulation results.","tags":[],"title":"On power law kernels, corresponding Reproducing Kernel Hilbert Space and applications","type":"publication"},{"authors":null,"categories":null,"content":"The project focussed on the problem of clustering complex networks that can be modelled as hypergraphs. Clustering hypergraphs has a long history, and a wide range of applications including motion segmentation, categorical data clustering and VLSI circuit partitioning. However, the theory of this problem was not well studied. My works are among the first studies on consistency of practical hypergraph clustering algorithms under a random planted community model. Along with proving theoretical guarantees, we also proposed efficient algorithms that achieve states-of-the-art results in motion segmentation and subspace clusterin\nCodes are available here\n","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"449f8f58a70598fae74972b2dcb2a7e0","permalink":"/project/hypergraph-clustering/","publishdate":"2013-01-01T00:00:00Z","relpermalink":"/project/hypergraph-clustering/","section":"project","summary":"2013-2016","tags":["machine-learning","network-analysis","computer-vision","statistics"],"title":"Clustering: From Subspaces to Hypergraphs","type":"project"},{"authors":["D. Ghoshdastidar","A. Dukkipati","S. Bhatnagar"],"categories":null,"content":"","date":1341100800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1341100800,"objectID":"0e395f3b77309b713d6b38a8b96116ef","permalink":"/publication/ghoshdastidar-isit-2012/","publishdate":"2012-07-01T00:00:00Z","relpermalink":"/publication/ghoshdastidar-isit-2012/","section":"publication","summary":"The q-Gaussian distribution results from maximizing certain generalizations of Shannon entropy under some constraints. The importance of q-Gaussian distributions stems from the fact that they exhibit power-law behavior, and also generalize Gaussian distributions. In this paper, we propose a Smoothed Functional (SF) scheme for gradient estimation using q-Gaussian distribution, and also propose an algorithm for optimization based on the above scheme. Convergence results of the algorithm are presented. Performance of the proposed algorithm is shown by simulation results on a queuing model.","tags":[],"title":"q-Gaussian based Smoothed Functional algorithms for stochastic optimization","type":"publication"},{"authors":null,"categories":null,"content":"Stochastic optimization is the mathematical tool for solving reinforcement learning problems, where the goal is to optimize the long-run performance of a dynamical system. We proposed new methods for stochastic optimization that estimate the gradient and Hessian of the cost function through a smoothing based on some generalised Gaussian distributions. The works provide convergence analysis and numerical performance of our algorithms.\n","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"ca799e4857546a35f10f7202a151de1b","permalink":"/project/optimization-qgaussian/","publishdate":"2011-01-01T00:00:00Z","relpermalink":"/project/optimization-qgaussian/","section":"project","summary":"2011-2013","tags":["optimization","information-theory"],"title":"Stochastic optimization with smoothing","type":"project"}]